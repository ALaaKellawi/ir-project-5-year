{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ALL IMPORTANT IMPORT",
   "id": "627904f88ec0976e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T21:12:56.819315Z",
     "start_time": "2024-06-03T21:12:56.798157Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import ir_datasets\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk import word_tokenize, PorterStemmer, pos_tag\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "# df = pd.read_csv(\"SecondData/second_data2.csv\")\n",
    "# df = df.fillna('')\n",
    "# print(df)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# df[\"embedding\"] = df.title.progress_apply(lambda x: model.encode(x))\n",
    "# df.to_parquet('./embedding_second2.pqt', index=False)"
   ],
   "id": "64a1bff9d4bc06c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\Ghost\\\\.ir_datasets\\\\lotte\\\\lifestyle\\\\dev\\\\collection.tsv\", sep='\\t')\n",
    "corpus = pd.DataFrame({\n",
    "    \"id\": df.iloc[:,0],\n",
    "    \"title\": df.iloc[:,1]\n",
    "})\n",
    "corpus.to_csv(\"./second_data2.csv\", index=False)"
   ],
   "id": "df198ef7c3a32d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "stop_words =[\"''\",\"``\",\"..\",\"n't\",\"'s\",\"u\",\"'m\",\"’\",\"'\",\"/\",\"...\",\".\",'”','””','wa','hi','doe','ha','whi',\"sometimes\",\"usually\",\"always\"]\n",
    "number_words = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen', 'nineteen', 'twenty']\n",
    "stop_words+= set(stopwords.words('english'))\n",
    "final_stop_words=stop_words\n",
    "stop_words2=stopwords.words('english')\n",
    "print(stop_words2)\n",
    "stemmer=PorterStemmer()\n",
    "import re\n",
    "class TextProcessor:\n",
    "    @staticmethod\n",
    "    def lower(text):\n",
    "         if isinstance(text, str):  # التحقق من أن القيمة من نوع str\n",
    "            return text.lower()\n",
    "         else:\n",
    "            return \"\"\n",
    "    @staticmethod\n",
    "    def stemming(text):\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(word) for word in text]   \n",
    "    \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def  tokenize_text(text):\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_stopwords(text):\n",
    "       return [word for word in text if not word in stop_words2]\n",
    "    \n",
    " \n",
    "    @staticmethod\n",
    "    def remove_punc(text):\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in text]\n",
    "        # print(\"remove punctuation:\", stripped)\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        return words\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_urls(text):\n",
    "        url_pattern = r\"http://[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&:/~\\+#]*[\\w\\-\\@?^=%&/~\\+#])?\"\n",
    "        # Replace URLs with an empty string\n",
    "        return re.sub(url_pattern, \"\", text)\n",
    "    @staticmethod\n",
    "    def remove_links( text):\n",
    "        import re\n",
    "        return re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_short_words(text):\n",
    "     min_length=3\n",
    "     words = text.split()\n",
    "     filtered_words = [word for word in words if len(word) >= min_length]\n",
    "     return ' '.join(filtered_words)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_number_words(text):\n",
    "     return [word for word in text if word.lower() not in number_words]\n",
    "  \n",
    "    @staticmethod\n",
    "    def get_wordnet_pos(tag_parameter):\n",
    "     tag = tag_parameter[0].upper()\n",
    "     tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "     return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    @staticmethod\n",
    "    def lemetize_Text(text):\n",
    "     pos_tags = nltk.pos_tag(text)\n",
    "     lemmatizer = WordNetLemmatizer()\n",
    "     lemmatized_words = [lemmatizer.lemmatize(text, pos=TextProcessor.get_wordnet_pos(pos_tag)) for text, pos_tag in pos_tags]\n",
    "     return lemmatized_words\n",
    "    \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def process(text):\n",
    "     text_processed = TextProcessor.lower(text)\n",
    "     text_processed = TextProcessor.remove_links( text_processed)\n",
    "     text_processed = TextProcessor.remove_number_words( text_processed)\n",
    "     text_processed = TextProcessor.remove_stopwords(text_processed)\n",
    "     text_processed = TextProcessor.tokenize_text(text_processed)\n",
    "     text_processed = TextProcessor.remove_short_words(text_processed)\n",
    "     text_processed = TextProcessor.remove_punc(text_processed)\n",
    "     text_processed = TextProcessor.lemetize_Text(text_processed)\n",
    "     # text_processed = TextProcessor.stemming(text_processed)\n",
    "     return text_processed\n",
    "\n",
    "    \n",
    "\n"
   ],
   "id": "e55acceb5406c453",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stop_words =[\"''\",\"``\",\"..\",\"n't\",\"'s\",\"u\",\"'m\",\"’\",\"'\",\"/\",\"...\",\".\",'”','””','wa','hi','doe','ha','whi',\"sometimes\",\"usually\",\"always\"]\n",
    "stop_words+= set(stopwords.words('english'))\n",
    "class TextProcessor2:\n",
    "    @staticmethod\n",
    "    def lower(text):\n",
    "         if isinstance(text, str):  # التحقق من أن القيمة من نوع str\n",
    "            return text.lower()\n",
    "         else:\n",
    "            return \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def     tokenize_text(text):\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_stopwords(text, stop_words=stop_words):\n",
    "       return [word for word in text if not word in stop_words]\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_number_words(text):\n",
    "     return [word for word in text if word.lower() not in number_words]\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_links( text):\n",
    "        import re\n",
    "        return re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_punc(text):\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in text]\n",
    "        # print(\"remove punctuation:\", stripped)\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        return words\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_short_word(text, min_length=3):\n",
    "        if isinstance(text, str):\n",
    "            filtered_words = [word for word in text.split() if len(word) >= min_length]\n",
    "            return ' '.join(filtered_words)\n",
    "        elif isinstance(text, list):\n",
    "            return [word for word in text if len(word) >= min_length]\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a string or a list of strings\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_wordnet_pos(tag_parameter):\n",
    "     tag = tag_parameter[0].upper()\n",
    "     tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "     return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def lemetize_Text(text):\n",
    "     pos_tags = nltk.pos_tag(text)\n",
    "     lemmatizer = WordNetLemmatizer()\n",
    "     lemmatized_words = [lemmatizer.lemmatize(text, pos=TextProcessor.get_wordnet_pos(pos_tag)) for text, pos_tag in pos_tags]\n",
    "     return lemmatized_words\n",
    "    \n",
    "    @staticmethod\n",
    "    def stemming(text):\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(word) for word in text]\n",
    "    @staticmethod\n",
    "    def process(text):\n",
    "        text_processed = TextProcessor2.remove_links(text)\n",
    "        text_processed = TextProcessor2.lower(text_processed)\n",
    "        text_processed = TextProcessor2.tokenize_text(text_processed)\n",
    "        text_processed = TextProcessor2.remove_number_words(text_processed)\n",
    "        text_processed = TextProcessor2.remove_stopwords(text_processed)\n",
    "        text_processed = TextProcessor2.remove_short_word(text_processed,3)\n",
    "        text_processed = TextProcessor2.remove_punc(text_processed)\n",
    "        text_processed = TextProcessor2.lemetize_Text(text_processed)\n",
    "        text_processed = TextProcessor2.stemming(text_processed)\n",
    "        return text_processed"
   ],
   "id": "1c85ec87e9fa4fb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"./second_data2.csv\")\n",
    "def clean_title(title):\n",
    "    processed_title = TextProcessor2.process(title)  # تنفيذ عمليات التنظيف على العنوان\n",
    "    cleaned_title = ' '.join(processed_title)  # تحويل العنوان المعالج إلى سلسلة نصية\n",
    "    return cleaned_title\n",
    "df['title'] = df['title'].apply(clean_title) \n",
    "df.to_csv('./cleaned_second_data3_new.csv', index=False) "
   ],
   "id": "16f49b96b1dda673",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def read_documents(directory):\n",
    "    documents = []\n",
    "    document_names = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename), 'r' ,encoding='utf-8') as file:\n",
    "            document = file.read()\n",
    "            documents.append(document)\n",
    "            document_names.append(filename)\n",
    "\n",
    "    return documents, document_names\n",
    "\n",
    "def read_documents_from_one_file(filePath):\n",
    "    df2 = pd.read_csv(filePath)\n",
    "    documents2 = df2['title'].fillna('', inplace=False).tolist()\n",
    "    document_names2 = df2['id'].tolist()\n",
    "    return documents2, document_names2\n",
    "def save_vectorizer(vectorizer, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "\n",
    "def save_tfidf_matrix(tfidf_matrix, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(tfidf_matrix, f)\n",
    "\n",
    "def save_document_names(document_names, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(document_names, f)\n",
    "\n",
    "def save_inverted_index(inverted_index, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(inverted_index, f)\n",
    "\n",
    "def load_vectorizer(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "# \n",
    "def load_tfidf_matrix(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_document_names(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ],
   "id": "ab1d2f522ada1d8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e54792cb4fbd4b87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "directory = \"./cleaned_second_data3_new.csv\"\n",
    "documents2, document_names2 = read_documents_from_one_file(directory)\n",
    "print(documents2)\n",
    "vectorizer = TfidfVectorizer()\n",
    "print(\"Vectorizer Created.\")\n",
    "tfidf_matrix = vectorizer.fit_transform(documents2)\n",
    "print(\"TF-IDF Created.\")\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# threshold = 0.01\n",
    "# feature_names = [feature_names[i] for i in range(len(feature_names)) if tfidf_matrix.toarray()[:,i].max() > threshold]\n",
    "# tfidf_matrix = tfidf_matrix[:, [i for i in range(len(feature_names)) if tfidf_matrix.toarray()[:,i].max() > threshold]]\n",
    "# os.makedirs(\"Users/Ghost/Desktop/pickleFiles/pickleFiles/1.pkl\", exist_ok=True)\n",
    "save_vectorizer(vectorizer, \"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\vectorizerDataset2New.pkl\")\n",
    "save_tfidf_matrix(tfidf_matrix, \"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\tfidf_matrixDataset2New.pkl\")\n",
    "save_document_names(document_names2, \"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\document_namesDataset2New.pkl\")"
   ],
   "id": "10fa51100bc469c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loaded_vectorizer = load_vectorizer(\"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\vectorizerDataset2New.pkl\")\n",
    "loaded_tfidf_matrix = load_tfidf_matrix(\"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\tfidf_matrixDataset2New.pkl\")\n",
    "loaded_document_names = load_document_names(\"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\document_namesDataset2New.pkl\")\n",
    "\n",
    "\n",
    "def query_matching(query: str, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names):\n",
    "    query2 = TextProcessor.process(query)\n",
    "    query3 = ' '.join(query2)\n",
    "    query_tfidf = loaded_vectorizer.transform([query3])\n",
    "    query_vector = query_tfidf\n",
    "    similarity = cosine_similarity(query_vector, loaded_tfidf_matrix).flatten()\n",
    "    # similarity1=np.argsort(similarity)[::-1]\n",
    "    matching_results = []\n",
    "    for i, doc_name in enumerate(loaded_document_names):\n",
    "        if similarity[i] > 0:\n",
    "            matching_results.append((doc_name, similarity[i]))\n",
    "    sorted_results = sorted(matching_results, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_results\n"
   ],
   "id": "8f60874b66345762",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def query_matching_top_10(query: str, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names):\n",
    "    query2 = TextProcessor2.process(query)\n",
    "    query3 = ' '.join(query2)\n",
    "    query_tfidf = loaded_vectorizer.transform([query3])\n",
    "    query_vector = query_tfidf\n",
    "    similarity = cosine_similarity(query_vector, loaded_tfidf_matrix).flatten()\n",
    "    matching_results = []\n",
    "    for i, doc_name in enumerate(loaded_document_names):\n",
    "        if similarity[i] > 0:\n",
    "            matching_results.append((doc_name, similarity[i]))\n",
    "    sorted_results = sorted(matching_results, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return only the top 10 results\n",
    "    return sorted_results[:10]"
   ],
   "id": "1b89bf175ea70bcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_queries(file_path, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names):\n",
    "    queries_df = pd.read_csv(file_path)\n",
    "    doc_names_list = []\n",
    "    query_id_results = {}\n",
    "    for index, row in queries_df.iterrows():\n",
    "        query_id = row['id']\n",
    "        query = row['title'].strip()\n",
    "        matching_results = query_matching_top_10(query, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names)\n",
    "        doc_names = [doc_name for doc_name, similarity in matching_results]\n",
    "        doc_names_list.extend(doc_names)\n",
    "        query_id_results[query_id] = {'Query': query, 'Document Names': doc_names}\n",
    "    query_results_df = pd.DataFrame.from_dict(query_id_results, orient='index')\n",
    "    query_results_df.to_csv('query_results_with_document_id3.csv', index_label='Query ID')\n",
    "file_path = './SecondDatasetQuery.csv'\n",
    "process_queries(file_path, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names)"
   ],
   "id": "c18c00ec7ecb9ee1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from typing import Dict\n",
    "\n",
    "def calculate_precision_at_k(qrels_file: str, system_output_file: str, k=10):\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    precision_at_k = {}\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        num_relevant = sum(1 for doc_id in doc_ids[:k] if str(doc_id) in relevant_docs)\n",
    "        precision_at_k[query_id] = num_relevant / k\n",
    "    \n",
    "    return precision_at_k\n",
    "\n",
    "# Example usage\n",
    "precision_at_10 = calculate_precision_at_k('secondqrels.csv', './second_query_with_doc_id_list', k=10)\n",
    "print(precision_at_10)"
   ],
   "id": "2231c30a9a79e1d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a19b0b98593bd803",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_queries_without_query_text(file_path, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names):\n",
    "    queries_df = pd.read_csv(file_path)\n",
    "    doc_names_list = []\n",
    "    query_id_results = {}\n",
    "    for index, row in queries_df.iterrows():\n",
    "        query_id = row['id']\n",
    "        query = row['title'].strip()\n",
    "        matching_results = query_matching_top_10(query, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names)\n",
    "        doc_names = [doc_name for doc_name, similarity in matching_results]\n",
    "        doc_names_list.extend(doc_names)\n",
    "        query_id_results[query_id] = {'doc_id': doc_names}\n",
    "    query_results_df = pd.DataFrame.from_dict(query_id_results, orient='index')\n",
    "    query_results_df.to_csv('second_query_with_doc_id_list2', index_label='Query ID')\n",
    "file_path = './SecondDatasetQuery.csv'\n",
    "process_queries_without_query_text(file_path, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names)"
   ],
   "id": "eed69b9886b6eae1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from typing import Dict\n",
    "\n",
    "def calculate_recall_at_k(qrels_file: str, system_output_file: str, k=10):\n",
    "    \n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    recall_at_k = {}\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        total_relevant = sum(1 for doc_id, relevance in relevant_docs.items() if relevance > 0)\n",
    "        num_relevant = sum(1 for doc_id in doc_ids[:k] if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0)\n",
    "        recall_at_k[query_id] = num_relevant / total_relevant\n",
    "    \n",
    "    return recall_at_k\n",
    "\n",
    "# Example usage\n",
    "recall_at_10 = calculate_recall_at_k('secondqrels.csv', './second_query_with_doc_id_list', k=10)\n",
    "print(recall_at_10)"
   ],
   "id": "93a6695139e12c77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MAP WITH TFIDF",
   "id": "1fb45d10a54940b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T21:14:09.551090Z",
     "start_time": "2024-06-03T21:14:09.525101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "def calculate_map(qrels_file: str, system_output_file: str) -> float:\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    average_precisions = []\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        total_relevant = sum(1 for doc_id, relevance in relevant_docs.items() if relevance > 0)\n",
    "        \n",
    "        if total_relevant == 0:\n",
    "            average_precisions.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        precision_sum = 0\n",
    "        num_relevant = 0\n",
    "        for rank, doc_id in enumerate(doc_ids, start=1):\n",
    "            if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0:\n",
    "                num_relevant += 1\n",
    "                precision = num_relevant / rank\n",
    "                precision_sum += precision\n",
    "        \n",
    "        average_precision = precision_sum / total_relevant\n",
    "        average_precisions.append(average_precision)\n",
    "    \n",
    "    map_value = sum(average_precisions) / len(average_precisions)\n",
    "    return map_value\n",
    "\n",
    "# Example usage\n",
    "map_value = calculate_map('secondqrels.csv', './second_query_with_doc_id_list')\n",
    "print(f\"MAP: {map_value}\")"
   ],
   "id": "a446ceb591b6acc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.16250355902477073\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "MAP WITH Embedding",
   "id": "285c20743f592b58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T21:13:20.489704Z",
     "start_time": "2024-06-03T21:13:20.447744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "def calculate_map(qrels_file: str, system_output_file: str) -> float:\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    average_precisions = []\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        total_relevant = sum(1 for doc_id, relevance in relevant_docs.items() if relevance > 0)\n",
    "        \n",
    "        if total_relevant == 0:\n",
    "            average_precisions.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        precision_sum = 0\n",
    "        num_relevant = 0\n",
    "        for rank, doc_id in enumerate(doc_ids, start=1):\n",
    "            if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0:\n",
    "                num_relevant += 1\n",
    "                precision = num_relevant / rank\n",
    "                precision_sum += precision\n",
    "        \n",
    "        average_precision = precision_sum / total_relevant\n",
    "        average_precisions.append(average_precision)\n",
    "    \n",
    "    map_value = sum(average_precisions) / len(average_precisions)\n",
    "    return map_value\n",
    "\n",
    "# Example usage\n",
    "map_value = calculate_map('secondqrels.csv', './lotte-query_result_without_cleaned_data.csv')\n",
    "print(f\"MAP: {map_value}\")"
   ],
   "id": "7a5568bcfe8d64f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.3892850498473144\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "def calculate_mrr(qrels_file: str, system_output_file: str) -> float:\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    mrr_values = []\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        \n",
    "        rank_of_first_relevant = float('inf')\n",
    "        for rank, doc_id in enumerate(doc_ids[:10], start=1):  # Consider only the top-10 results\n",
    "            if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0:\n",
    "                rank_of_first_relevant = rank\n",
    "                break\n",
    "        \n",
    "        if rank_of_first_relevant == float('inf'):\n",
    "            mrr_values.append(0.0)\n",
    "        else:\n",
    "            mrr_values.append(1 / rank_of_first_relevant)\n",
    "    \n",
    "    mrr_value = sum(mrr_values) / len(mrr_values)\n",
    "    return mrr_value\n",
    "\n",
    "# Example usage\n",
    "mrr_value = calculate_mrr('secondqrels.csv', './second_query_with_doc_id_list2')\n",
    "print(f\"MRR: {mrr_value}\")"
   ],
   "id": "fabac938d8dde0f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "def calculate_precision_at_10(qrels_file: str, system_output_file: str) -> float:\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    precision_values = []\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        \n",
    "        num_relevant_in_top_10 = 0\n",
    "        for rank, doc_id in enumerate(doc_ids[:10], start=1):\n",
    "            if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0:\n",
    "                num_relevant_in_top_10 += 1\n",
    "        \n",
    "        precision_at_10 = num_relevant_in_top_10 / 10\n",
    "        precision_values.append(precision_at_10)\n",
    "    \n",
    "    precision_at_10 = sum(precision_values) / len(precision_values)\n",
    "    return precision_at_10\n",
    "\n",
    "# Example usage\n",
    "precision_at_10 = calculate_precision_at_10('secondqrels.csv', './second_query_with_doc_id_list2')\n",
    "print(f\"Precision at 10: {precision_at_10}\")"
   ],
   "id": "dcb1d2f183e15211",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "def calculate_recall_at_10(qrels_file: str, system_output_file: str) -> float:\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    recall_values = []\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        total_relevant_docs = sum(relevant_docs.values())\n",
    "        \n",
    "        num_relevant_in_top_10 = 0\n",
    "        for rank, doc_id in enumerate(doc_ids[:10], start=1):\n",
    "            if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0:\n",
    "                num_relevant_in_top_10 += 1\n",
    "        \n",
    "        recall_at_10 = num_relevant_in_top_10 / total_relevant_docs\n",
    "        recall_values.append(recall_at_10)\n",
    "    \n",
    "    recall_at_10 = sum(recall_values) / len(recall_values)\n",
    "    return recall_at_10\n",
    "\n",
    "# Example usage\n",
    "recall_at_10 = calculate_recall_at_10('secondqrels.csv', './second_query_with_doc_id_list2')\n",
    "print(f\"Recall at 10: {recall_at_10}\")"
   ],
   "id": "741b3654ce045ecd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "177a283b812aff3e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
