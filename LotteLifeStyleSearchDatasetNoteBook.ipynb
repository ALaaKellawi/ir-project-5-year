{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ALL IMPORTANT IMPORT",
   "id": "627904f88ec0976e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-02T14:06:24.368512Z",
     "start_time": "2024-06-02T14:06:21.720794Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import ir_datasets\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk import word_tokenize, PorterStemmer, pos_tag\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as npimport pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "\n",
    "df = pd.read_csv(\"SecondData/second_data2.csv\")\n",
    "df = df.fillna('')\n",
    "print(df)\n",
    "\n",
    "df[\"embedding\"] = df.title.progress_apply(lambda x: model.encode(x))\n",
    "df.to_parquet('./embedding_second2.pqt', index=False)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T08:08:46.501662Z",
     "start_time": "2024-05-21T08:08:32.755168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\Ghost\\\\.ir_datasets\\\\lotte\\\\lifestyle\\\\dev\\\\collection.tsv\", sep='\\t')\n",
    "corpus = pd.DataFrame({\n",
    "    \"id\": df.iloc[:,0],\n",
    "    \"title\": df.iloc[:,1]\n",
    "})\n",
    "corpus.to_csv(\"./second_data2.csv\", index=False)"
   ],
   "id": "df198ef7c3a32d7",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T22:33:18.143610Z",
     "start_time": "2024-05-22T22:33:18.125216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "stop_words =[\"''\",\"``\",\"..\",\"n't\",\"'s\",\"u\",\"'m\",\"’\",\"'\",\"/\",\"...\",\".\",'”','””','wa','hi','doe','ha','whi',\"sometimes\",\"usually\",\"always\"]\n",
    "number_words = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen', 'nineteen', 'twenty']\n",
    "stop_words+= set(stopwords.words('english'))\n",
    "final_stop_words=stop_words\n",
    "stop_words2=stopwords.words('english')\n",
    "print(stop_words2)\n",
    "stemmer=PorterStemmer()\n",
    "import re\n",
    "class TextProcessor:\n",
    "    @staticmethod\n",
    "    def lower(text):\n",
    "         if isinstance(text, str):  # التحقق من أن القيمة من نوع str\n",
    "            return text.lower()\n",
    "         else:\n",
    "            return \"\"\n",
    "    @staticmethod\n",
    "    def stemming(text):\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(word) for word in text]   \n",
    "    \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def  tokenize_text(text):\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_stopwords(text):\n",
    "       return [word for word in text if not word in stop_words2]\n",
    "    \n",
    " \n",
    "    @staticmethod\n",
    "    def remove_punc(text):\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in text]\n",
    "        # print(\"remove punctuation:\", stripped)\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        return words\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_urls(text):\n",
    "        url_pattern = r\"http://[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&:/~\\+#]*[\\w\\-\\@?^=%&/~\\+#])?\"\n",
    "        # Replace URLs with an empty string\n",
    "        return re.sub(url_pattern, \"\", text)\n",
    "    @staticmethod\n",
    "    def remove_links( text):\n",
    "        import re\n",
    "        return re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_short_words(text):\n",
    "     min_length=3\n",
    "     words = text.split()\n",
    "     filtered_words = [word for word in words if len(word) >= min_length]\n",
    "     return ' '.join(filtered_words)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_number_words(text):\n",
    "     return [word for word in text if word.lower() not in number_words]\n",
    "  \n",
    "    @staticmethod\n",
    "    def get_wordnet_pos(tag_parameter):\n",
    "     tag = tag_parameter[0].upper()\n",
    "     tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "     return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    @staticmethod\n",
    "    def lemetize_Text(text):\n",
    "     pos_tags = nltk.pos_tag(text)\n",
    "     lemmatizer = WordNetLemmatizer()\n",
    "     lemmatized_words = [lemmatizer.lemmatize(text, pos=TextProcessor.get_wordnet_pos(pos_tag)) for text, pos_tag in pos_tags]\n",
    "     return lemmatized_words\n",
    "    \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def process(text):\n",
    "     text_processed = TextProcessor.lower(text)\n",
    "     text_processed = TextProcessor.remove_links( text_processed)\n",
    "     text_processed = TextProcessor.remove_number_words( text_processed)\n",
    "     text_processed = TextProcessor.remove_stopwords(text_processed)\n",
    "     text_processed = TextProcessor.tokenize_text(text_processed)\n",
    "     text_processed = TextProcessor.remove_short_words(text_processed)\n",
    "     text_processed = TextProcessor.remove_punc(text_processed)\n",
    "     text_processed = TextProcessor.lemetize_Text(text_processed)\n",
    "     # text_processed = TextProcessor.stemming(text_processed)\n",
    "     return text_processed\n",
    "\n",
    "    \n",
    "\n"
   ],
   "id": "e55acceb5406c453",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T07:07:40.663102Z",
     "start_time": "2024-05-29T07:07:40.631315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stop_words =[\"''\",\"``\",\"..\",\"n't\",\"'s\",\"u\",\"'m\",\"’\",\"'\",\"/\",\"...\",\".\",'”','””','wa','hi','doe','ha','whi',\"sometimes\",\"usually\",\"always\"]\n",
    "stop_words+= set(stopwords.words('english'))\n",
    "class TextProcessor2:\n",
    "    @staticmethod\n",
    "    def lower(text):\n",
    "         if isinstance(text, str):  # التحقق من أن القيمة من نوع str\n",
    "            return text.lower()\n",
    "         else:\n",
    "            return \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def     tokenize_text(text):\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_stopwords(text, stop_words=stop_words):\n",
    "       return [word for word in text if not word in stop_words]\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_number_words(text):\n",
    "     return [word for word in text if word.lower() not in number_words]\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_links( text):\n",
    "        import re\n",
    "        return re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_punc(text):\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in text]\n",
    "        # print(\"remove punctuation:\", stripped)\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        return words\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_short_word(text, min_length=3):\n",
    "        if isinstance(text, str):\n",
    "            filtered_words = [word for word in text.split() if len(word) >= min_length]\n",
    "            return ' '.join(filtered_words)\n",
    "        elif isinstance(text, list):\n",
    "            return [word for word in text if len(word) >= min_length]\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a string or a list of strings\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_wordnet_pos(tag_parameter):\n",
    "     tag = tag_parameter[0].upper()\n",
    "     tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "     return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def lemetize_Text(text):\n",
    "     pos_tags = nltk.pos_tag(text)\n",
    "     lemmatizer = WordNetLemmatizer()\n",
    "     lemmatized_words = [lemmatizer.lemmatize(text, pos=TextProcessor.get_wordnet_pos(pos_tag)) for text, pos_tag in pos_tags]\n",
    "     return lemmatized_words\n",
    "    \n",
    "    @staticmethod\n",
    "    def stemming(text):\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(word) for word in text]\n",
    "    @staticmethod\n",
    "    def process(text):\n",
    "        text_processed = TextProcessor2.remove_links(text)\n",
    "        text_processed = TextProcessor2.lower(text_processed)\n",
    "        text_processed = TextProcessor2.tokenize_text(text_processed)\n",
    "        text_processed = TextProcessor2.remove_number_words(text_processed)\n",
    "        text_processed = TextProcessor2.remove_stopwords(text_processed)\n",
    "        text_processed = TextProcessor2.remove_short_word(text_processed,3)\n",
    "        text_processed = TextProcessor2.remove_punc(text_processed)\n",
    "        text_processed = TextProcessor2.lemetize_Text(text_processed)\n",
    "        text_processed = TextProcessor2.stemming(text_processed)\n",
    "        return text_processed"
   ],
   "id": "1c85ec87e9fa4fb8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T23:27:00.959290Z",
     "start_time": "2024-05-22T22:46:37.947759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"./second_data2.csv\")\n",
    "def clean_title(title):\n",
    "    processed_title = TextProcessor2.process(title)  # تنفيذ عمليات التنظيف على العنوان\n",
    "    cleaned_title = ' '.join(processed_title)  # تحويل العنوان المعالج إلى سلسلة نصية\n",
    "    return cleaned_title\n",
    "df['title'] = df['title'].apply(clean_title) \n",
    "df.to_csv('./cleaned_second_data3_new.csv', index=False) "
   ],
   "id": "16f49b96b1dda673",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T07:30:43.924559Z",
     "start_time": "2024-05-24T07:30:43.915716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_documents(directory):\n",
    "    documents = []\n",
    "    document_names = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename), 'r' ,encoding='utf-8') as file:\n",
    "            document = file.read()\n",
    "            documents.append(document)\n",
    "            document_names.append(filename)\n",
    "\n",
    "    return documents, document_names\n",
    "\n",
    "def read_documents_from_one_file(filePath):\n",
    "    df2 = pd.read_csv(filePath)\n",
    "    documents2 = df2['title'].fillna('', inplace=False).tolist()\n",
    "    document_names2 = df2['id'].tolist()\n",
    "    return documents2, document_names2\n",
    "def save_vectorizer(vectorizer, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "\n",
    "def save_tfidf_matrix(tfidf_matrix, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(tfidf_matrix, f)\n",
    "\n",
    "def save_document_names(document_names, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(document_names, f)\n",
    "\n",
    "def save_inverted_index(inverted_index, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(inverted_index, f)\n",
    "\n",
    "def load_vectorizer(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "# \n",
    "def load_tfidf_matrix(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_document_names(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ],
   "id": "ab1d2f522ada1d8b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e54792cb4fbd4b87"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-23T04:38:34.946336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "directory = \"./cleaned_second_data3_new.csv\"\n",
    "documents2, document_names2 = read_documents_from_one_file(directory)\n",
    "print(documents2)\n",
    "vectorizer = TfidfVectorizer()\n",
    "print(\"Vectorizer Created.\")\n",
    "tfidf_matrix = vectorizer.fit_transform(documents2)\n",
    "print(\"TF-IDF Created.\")\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# threshold = 0.01\n",
    "# feature_names = [feature_names[i] for i in range(len(feature_names)) if tfidf_matrix.toarray()[:,i].max() > threshold]\n",
    "# tfidf_matrix = tfidf_matrix[:, [i for i in range(len(feature_names)) if tfidf_matrix.toarray()[:,i].max() > threshold]]\n",
    "# os.makedirs(\"Users/Ghost/Desktop/pickleFiles/pickleFiles/1.pkl\", exist_ok=True)\n",
    "save_vectorizer(vectorizer, \"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\vectorizerDataset2New.pkl\")\n",
    "save_tfidf_matrix(tfidf_matrix, \"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\tfidf_matrixDataset2New.pkl\")\n",
    "save_document_names(document_names2, \"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\document_namesDataset2New.pkl\")"
   ],
   "id": "10fa51100bc469c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T07:30:47.472422Z",
     "start_time": "2024-05-24T07:30:47.106728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loaded_vectorizer = load_vectorizer(\"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\vectorizerDataset2New.pkl\")\n",
    "loaded_tfidf_matrix = load_tfidf_matrix(\"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\tfidf_matrixDataset2New.pkl\")\n",
    "loaded_document_names = load_document_names(\"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\document_namesDataset2New.pkl\")\n",
    "\n",
    "\n",
    "def query_matching(query: str, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names):\n",
    "    query2 = TextProcessor.process(query)\n",
    "    query3 = ' '.join(query2)\n",
    "    query_tfidf = loaded_vectorizer.transform([query3])\n",
    "    query_vector = query_tfidf\n",
    "    similarity = cosine_similarity(query_vector, loaded_tfidf_matrix).flatten()\n",
    "    # similarity1=np.argsort(similarity)[::-1]\n",
    "    matching_results = []\n",
    "    for i, doc_name in enumerate(loaded_document_names):\n",
    "        if similarity[i] > 0:\n",
    "            matching_results.append((doc_name, similarity[i]))\n",
    "    sorted_results = sorted(matching_results, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_results\n"
   ],
   "id": "8f60874b66345762",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T07:30:51.793328Z",
     "start_time": "2024-05-24T07:30:51.787314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def query_matching_top_10(query: str, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names):\n",
    "    query2 = TextProcessor2.process(query)\n",
    "    query3 = ' '.join(query2)\n",
    "    query_tfidf = loaded_vectorizer.transform([query3])\n",
    "    query_vector = query_tfidf\n",
    "    similarity = cosine_similarity(query_vector, loaded_tfidf_matrix).flatten()\n",
    "    matching_results = []\n",
    "    for i, doc_name in enumerate(loaded_document_names):\n",
    "        if similarity[i] > 0:\n",
    "            matching_results.append((doc_name, similarity[i]))\n",
    "    sorted_results = sorted(matching_results, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return only the top 10 results\n",
    "    return sorted_results[:10]"
   ],
   "id": "1b89bf175ea70bcc",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T10:32:07.670113Z",
     "start_time": "2024-05-21T10:26:14.914546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_queries(file_path, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names):\n",
    "    queries_df = pd.read_csv(file_path)\n",
    "    doc_names_list = []\n",
    "    query_id_results = {}\n",
    "    for index, row in queries_df.iterrows():\n",
    "        query_id = row['id']\n",
    "        query = row['title'].strip()\n",
    "        matching_results = query_matching_top_10(query, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names)\n",
    "        doc_names = [doc_name for doc_name, similarity in matching_results]\n",
    "        doc_names_list.extend(doc_names)\n",
    "        query_id_results[query_id] = {'Query': query, 'Document Names': doc_names}\n",
    "    query_results_df = pd.DataFrame.from_dict(query_id_results, orient='index')\n",
    "    query_results_df.to_csv('query_results_with_document_id3.csv', index_label='Query ID')\n",
    "file_path = './SecondDatasetQuery.csv'\n",
    "process_queries(file_path, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names)"
   ],
   "id": "c18c00ec7ecb9ee1",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T10:52:07.655193Z",
     "start_time": "2024-05-21T10:52:07.632838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from typing import Dict\n",
    "\n",
    "def calculate_precision_at_k(qrels_file: str, system_output_file: str, k=10):\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    precision_at_k = {}\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        num_relevant = sum(1 for doc_id in doc_ids[:k] if str(doc_id) in relevant_docs)\n",
    "        precision_at_k[query_id] = num_relevant / k\n",
    "    \n",
    "    return precision_at_k\n",
    "\n",
    "# Example usage\n",
    "precision_at_10 = calculate_precision_at_k('secondqrels.csv', './second_query_with_doc_id_list', k=10)\n",
    "print(precision_at_10)"
   ],
   "id": "2231c30a9a79e1d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0.2, '1': 0.4, '2': 0.2, '3': 0.1, '4': 0.2, '5': 0.1, '6': 0.1, '7': 0.1, '8': 0.5, '9': 0.1, '10': 0.1, '11': 0.1, '12': 0.1, '13': 0.0, '14': 0.5, '15': 0.1, '16': 0.0, '17': 0.0, '18': 0.1, '19': 0.1, '20': 0.0, '21': 0.0, '22': 0.1, '23': 0.1, '24': 0.0, '25': 0.0, '26': 0.0, '27': 0.0, '28': 0.0, '29': 0.0, '30': 0.0, '31': 0.2, '32': 0.3, '33': 0.1, '34': 0.0, '35': 0.1, '36': 0.0, '37': 0.0, '38': 0.1, '39': 0.1, '40': 0.0, '41': 0.0, '42': 0.1, '43': 0.1, '44': 0.0, '45': 0.2, '46': 0.1, '47': 0.1, '48': 0.2, '49': 0.2, '50': 0.0, '51': 0.2, '52': 0.3, '53': 0.1, '54': 0.3, '55': 0.2, '56': 0.0, '57': 0.3, '58': 0.0, '59': 0.2, '60': 0.2, '61': 0.2, '62': 0.1, '63': 0.1, '64': 0.4, '65': 0.3, '66': 0.0, '67': 0.0, '68': 0.1, '69': 0.2, '70': 0.2, '71': 0.1, '72': 0.1, '73': 0.1, '74': 0.1, '75': 0.0, '76': 0.1, '77': 0.1, '78': 0.1, '79': 0.0, '80': 0.0, '81': 0.3, '82': 0.0, '83': 0.2, '84': 0.1, '85': 0.3, '86': 0.2, '87': 0.1, '88': 0.1, '89': 0.1, '90': 0.1, '91': 0.1, '92': 0.1, '93': 0.3, '94': 0.3, '95': 0.2, '96': 0.1, '97': 0.4, '98': 0.1, '99': 0.1, '100': 0.0, '101': 0.0, '102': 0.1, '103': 0.0, '104': 0.1, '105': 0.2, '106': 0.0, '107': 0.0, '108': 0.1, '109': 0.1, '110': 0.2, '111': 0.4, '112': 0.1, '113': 0.3, '114': 0.1, '115': 0.0, '116': 0.0, '117': 0.1, '118': 0.0, '119': 0.0, '120': 0.1, '121': 0.1, '122': 0.0, '123': 0.0, '124': 0.0, '125': 0.0, '126': 0.0, '127': 0.1, '128': 0.0, '129': 0.2, '130': 0.2, '131': 0.0, '132': 0.3, '133': 0.0, '134': 0.2, '135': 0.1, '136': 0.1, '137': 0.1, '138': 0.0, '139': 0.0, '140': 0.0, '141': 0.2, '142': 0.0, '143': 0.2, '144': 0.0, '145': 0.0, '146': 0.1, '147': 0.0, '148': 0.0, '149': 0.0, '150': 0.0, '151': 0.1, '152': 0.2, '153': 0.1, '154': 0.0, '155': 0.2, '156': 0.0, '157': 0.0, '158': 0.0, '159': 0.1, '160': 0.0, '161': 0.2, '162': 0.0, '163': 0.1, '164': 0.1, '165': 0.0, '166': 0.1, '167': 0.1, '168': 0.0, '169': 0.0, '170': 0.0, '171': 0.0, '172': 0.0, '173': 0.0, '174': 0.1, '175': 0.2, '176': 0.0, '177': 0.1, '178': 0.3, '179': 0.1, '180': 0.1, '181': 0.0, '182': 0.2, '183': 0.0, '184': 0.1, '185': 0.0, '186': 0.0, '187': 0.1, '188': 0.0, '189': 0.0, '190': 0.0, '191': 0.0, '192': 0.0, '193': 0.0, '194': 0.1, '195': 0.0, '196': 0.0, '197': 0.0, '198': 0.1, '199': 0.0, '200': 0.0, '201': 0.0, '202': 0.1, '203': 0.0, '204': 0.0, '205': 0.0, '206': 0.0, '207': 0.1, '208': 0.2, '209': 0.0, '210': 0.1, '211': 0.2, '212': 0.2, '213': 0.0, '214': 0.0, '215': 0.1, '216': 0.0, '217': 0.0, '218': 0.2, '219': 0.0, '220': 0.0, '221': 0.1, '222': 0.0, '223': 0.0, '224': 0.0, '225': 0.0, '226': 0.0, '227': 0.0, '228': 0.0, '229': 0.1, '230': 0.2, '231': 0.0, '232': 0.1, '233': 0.0, '234': 0.0, '235': 0.3, '236': 0.0, '237': 0.0, '238': 0.1, '239': 0.1, '240': 0.0, '241': 0.0, '242': 0.0, '243': 0.0, '244': 0.1, '245': 0.0, '246': 0.1, '247': 0.0, '248': 0.1, '249': 0.0, '250': 0.0, '251': 0.1, '252': 0.1, '253': 0.0, '254': 0.1, '255': 0.0, '256': 0.0, '257': 0.1, '258': 0.0, '259': 0.0, '260': 0.1, '261': 0.0, '262': 0.1, '263': 0.0, '264': 0.1, '265': 0.2, '266': 0.1, '267': 0.1, '268': 0.0, '269': 0.3, '270': 0.0, '271': 0.0, '272': 0.2, '273': 0.0, '274': 0.0, '275': 0.0, '276': 0.1, '277': 0.2, '278': 0.1, '279': 0.2, '280': 0.2, '281': 0.1, '282': 0.0, '283': 0.0, '284': 0.1, '285': 0.0, '286': 0.2, '287': 0.0, '288': 0.0, '289': 0.0, '290': 0.1, '291': 0.0, '292': 0.1, '293': 0.0, '294': 0.0, '295': 0.0, '296': 0.1, '297': 0.1, '298': 0.1, '299': 0.0, '300': 0.2, '301': 0.1, '302': 0.0, '303': 0.0, '304': 0.1, '305': 0.0, '306': 0.0, '307': 0.1, '308': 0.2, '309': 0.0, '310': 0.2, '311': 0.0, '312': 0.1, '313': 0.0, '314': 0.0, '315': 0.0, '316': 0.0, '317': 0.0, '318': 0.0, '319': 0.0, '320': 0.1, '321': 0.0, '322': 0.3, '323': 0.1, '324': 0.1, '325': 0.0, '326': 0.0, '327': 0.0, '328': 0.0, '329': 0.0, '330': 0.3, '331': 0.0, '332': 0.1, '333': 0.3, '334': 0.0, '335': 0.1, '336': 0.2, '337': 0.0, '338': 0.1, '339': 0.0, '340': 0.1, '341': 0.3, '342': 0.0, '343': 0.1, '344': 0.1, '345': 0.1, '346': 0.3, '347': 0.0, '348': 0.0, '349': 0.0, '350': 0.0, '351': 0.3, '352': 0.1, '353': 0.0, '354': 0.1, '355': 0.2, '356': 0.1, '357': 0.0, '358': 0.3, '359': 0.0, '360': 0.1, '361': 0.0, '362': 0.2, '363': 0.0, '364': 0.0, '365': 0.0, '366': 0.0, '367': 0.0, '368': 0.1, '369': 0.0, '370': 0.1, '371': 0.1, '372': 0.1, '373': 0.0, '374': 0.0, '375': 0.0, '376': 0.0, '377': 0.1, '378': 0.0, '379': 0.0, '380': 0.0, '381': 0.0, '382': 0.4, '383': 0.0, '384': 0.0, '385': 0.2, '386': 0.0, '387': 0.0, '388': 0.1, '389': 0.0, '390': 0.3, '391': 0.0, '392': 0.1, '393': 0.0, '394': 0.1, '395': 0.0, '396': 0.3, '397': 0.0, '398': 0.1, '399': 0.1, '400': 0.2, '401': 0.0, '402': 0.0, '403': 0.1, '404': 0.0, '405': 0.1, '406': 0.3, '407': 0.1, '408': 0.0, '409': 0.0, '410': 0.0, '411': 0.0, '412': 0.2, '413': 0.3, '414': 0.0, '415': 0.0, '416': 0.0}\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T10:42:48.733306Z",
     "start_time": "2024-05-21T10:42:48.631582Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a19b0b98593bd803",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T17:08:57.085704Z",
     "start_time": "2024-05-22T17:02:59.187610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_queries_without_query_text(file_path, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names):\n",
    "    queries_df = pd.read_csv(file_path)\n",
    "    doc_names_list = []\n",
    "    query_id_results = {}\n",
    "    for index, row in queries_df.iterrows():\n",
    "        query_id = row['id']\n",
    "        query = row['title'].strip()\n",
    "        matching_results = query_matching_top_10(query, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names)\n",
    "        doc_names = [doc_name for doc_name, similarity in matching_results]\n",
    "        doc_names_list.extend(doc_names)\n",
    "        query_id_results[query_id] = {'doc_id': doc_names}\n",
    "    query_results_df = pd.DataFrame.from_dict(query_id_results, orient='index')\n",
    "    query_results_df.to_csv('second_query_with_doc_id_list2', index_label='Query ID')\n",
    "file_path = './SecondDatasetQuery.csv'\n",
    "process_queries_without_query_text(file_path, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names)"
   ],
   "id": "eed69b9886b6eae1",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T10:56:18.189858Z",
     "start_time": "2024-05-21T10:56:18.173193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from typing import Dict\n",
    "\n",
    "def calculate_recall_at_k(qrels_file: str, system_output_file: str, k=10):\n",
    "    \n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    recall_at_k = {}\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        total_relevant = sum(1 for doc_id, relevance in relevant_docs.items() if relevance > 0)\n",
    "        num_relevant = sum(1 for doc_id in doc_ids[:k] if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0)\n",
    "        recall_at_k[query_id] = num_relevant / total_relevant\n",
    "    \n",
    "    return recall_at_k\n",
    "\n",
    "# Example usage\n",
    "recall_at_10 = calculate_recall_at_k('secondqrels.csv', './second_query_with_doc_id_list', k=10)\n",
    "print(recall_at_10)"
   ],
   "id": "93a6695139e12c77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0.4, '1': 0.8, '2': 0.5, '3': 1.0, '4': 0.4, '5': 1.0, '6': 0.5, '7': 0.25, '8': 0.625, '9': 0.5, '10': 0.5, '11': 1.0, '12': 1.0, '13': 0.0, '14': 0.625, '15': 1.0, '16': 0.0, '17': 0.0, '18': 0.3333333333333333, '19': 1.0, '20': 0.0, '21': 0.0, '22': 1.0, '23': 1.0, '24': 0.0, '25': 0.0, '26': 0.0, '27': 0.0, '28': 0.0, '29': 0.0, '30': 0.0, '31': 0.2857142857142857, '32': 0.75, '33': 1.0, '34': 0.0, '35': 0.5, '36': 0.0, '37': 0.0, '38': 0.5, '39': 0.5, '40': 0.0, '41': 0.0, '42': 0.16666666666666666, '43': 0.3333333333333333, '44': 0.0, '45': 1.0, '46': 0.1, '47': 0.5, '48': 1.0, '49': 1.0, '50': 0.0, '51': 1.0, '52': 1.0, '53': 0.5, '54': 1.0, '55': 1.0, '56': 0.0, '57': 0.6, '58': 0.0, '59': 1.0, '60': 0.6666666666666666, '61': 0.6666666666666666, '62': 1.0, '63': 0.14285714285714285, '64': 1.0, '65': 0.42857142857142855, '66': 0.0, '67': 0.0, '68': 0.5, '69': 1.0, '70': 1.0, '71': 1.0, '72': 0.5, '73': 0.5, '74': 0.25, '75': 0.0, '76': 1.0, '77': 0.5, '78': 1.0, '79': 0.0, '80': 0.0, '81': 0.5, '82': 0.0, '83': 0.14285714285714285, '84': 0.16666666666666666, '85': 0.6, '86': 0.5, '87': 0.3333333333333333, '88': 0.3333333333333333, '89': 0.5, '90': 0.05555555555555555, '91': 0.5, '92': 0.25, '93': 0.75, '94': 1.0, '95': 0.2857142857142857, '96': 0.5, '97': 1.0, '98': 1.0, '99': 0.5, '100': 0.0, '101': 0.0, '102': 0.3333333333333333, '103': 0.0, '104': 0.2, '105': 0.15384615384615385, '106': 0.0, '107': 0.0, '108': 0.25, '109': 0.25, '110': 0.5, '111': 1.0, '112': 0.16666666666666666, '113': 1.0, '114': 0.1111111111111111, '115': 0.0, '116': 0.0, '117': 0.25, '118': 0.0, '119': 0.0, '120': 0.25, '121': 0.16666666666666666, '122': 0.0, '123': 0.0, '124': 0.0, '125': 0.0, '126': 0.0, '127': 0.25, '128': 0.0, '129': 0.6666666666666666, '130': 0.3333333333333333, '131': 0.0, '132': 1.0, '133': 0.0, '134': 0.6666666666666666, '135': 1.0, '136': 0.25, '137': 0.25, '138': 0.0, '139': 0.0, '140': 0.0, '141': 0.6666666666666666, '142': 0.0, '143': 0.4, '144': 0.0, '145': 0.0, '146': 0.3333333333333333, '147': 0.0, '148': 0.0, '149': 0.0, '150': 0.0, '151': 0.16666666666666666, '152': 1.0, '153': 1.0, '154': 0.0, '155': 0.25, '156': 0.0, '157': 0.0, '158': 0.0, '159': 0.5, '160': 0.0, '161': 1.0, '162': 0.0, '163': 0.3333333333333333, '164': 0.3333333333333333, '165': 0.0, '166': 0.3333333333333333, '167': 0.3333333333333333, '168': 0.0, '169': 0.0, '170': 0.0, '171': 0.0, '172': 0.0, '173': 0.0, '174': 0.125, '175': 1.0, '176': 0.0, '177': 0.16666666666666666, '178': 0.75, '179': 1.0, '180': 0.2, '181': 0.0, '182': 1.0, '183': 0.0, '184': 0.3333333333333333, '185': 0.0, '186': 0.0, '187': 0.3333333333333333, '188': 0.0, '189': 0.0, '190': 0.0, '191': 0.0, '192': 0.0, '193': 0.0, '194': 0.3333333333333333, '195': 0.0, '196': 0.0, '197': 0.0, '198': 0.5, '199': 0.0, '200': 0.0, '201': 0.0, '202': 0.5, '203': 0.0, '204': 0.0, '205': 0.0, '206': 0.0, '207': 0.5, '208': 1.0, '209': 0.0, '210': 0.5, '211': 0.5, '212': 0.4, '213': 0.0, '214': 0.0, '215': 0.16666666666666666, '216': 0.0, '217': 0.0, '218': 1.0, '219': 0.0, '220': 0.0, '221': 0.08333333333333333, '222': 0.0, '223': 0.0, '224': 0.0, '225': 0.0, '226': 0.0, '227': 0.0, '228': 0.0, '229': 0.25, '230': 0.4, '231': 0.0, '232': 0.25, '233': 0.0, '234': 0.0, '235': 0.75, '236': 0.0, '237': 0.0, '238': 0.3333333333333333, '239': 1.0, '240': 0.0, '241': 0.0, '242': 0.0, '243': 0.0, '244': 0.3333333333333333, '245': 0.0, '246': 0.5, '247': 0.0, '248': 0.5, '249': 0.0, '250': 0.0, '251': 0.2, '252': 0.3333333333333333, '253': 0.0, '254': 0.5, '255': 0.0, '256': 0.0, '257': 0.3333333333333333, '258': 0.0, '259': 0.0, '260': 0.125, '261': 0.0, '262': 0.3333333333333333, '263': 0.0, '264': 1.0, '265': 0.5, '266': 1.0, '267': 0.3333333333333333, '268': 0.0, '269': 0.5, '270': 0.0, '271': 0.0, '272': 1.0, '273': 0.0, '274': 0.0, '275': 0.0, '276': 0.3333333333333333, '277': 1.0, '278': 0.2, '279': 0.6666666666666666, '280': 0.16666666666666666, '281': 0.16666666666666666, '282': 0.0, '283': 0.0, '284': 0.25, '285': 0.0, '286': 0.25, '287': 0.0, '288': 0.0, '289': 0.0, '290': 0.25, '291': 0.0, '292': 0.5, '293': 0.0, '294': 0.0, '295': 0.0, '296': 0.09090909090909091, '297': 0.25, '298': 0.3333333333333333, '299': 0.0, '300': 0.6666666666666666, '301': 0.5, '302': 0.0, '303': 0.0, '304': 0.3333333333333333, '305': 0.0, '306': 0.0, '307': 0.3333333333333333, '308': 0.2222222222222222, '309': 0.0, '310': 0.6666666666666666, '311': 0.0, '312': 0.5, '313': 0.0, '314': 0.0, '315': 0.0, '316': 0.0, '317': 0.0, '318': 0.0, '319': 0.0, '320': 0.5, '321': 0.0, '322': 0.75, '323': 0.2, '324': 0.5, '325': 0.0, '326': 0.0, '327': 0.0, '328': 0.0, '329': 0.0, '330': 0.6, '331': 0.0, '332': 1.0, '333': 1.0, '334': 0.0, '335': 0.25, '336': 0.5, '337': 0.0, '338': 1.0, '339': 0.0, '340': 1.0, '341': 0.75, '342': 0.0, '343': 1.0, '344': 0.16666666666666666, '345': 1.0, '346': 0.75, '347': 0.0, '348': 0.0, '349': 0.0, '350': 0.0, '351': 1.0, '352': 0.3333333333333333, '353': 0.0, '354': 0.125, '355': 0.6666666666666666, '356': 1.0, '357': 0.0, '358': 1.0, '359': 0.0, '360': 0.5, '361': 0.0, '362': 1.0, '363': 0.0, '364': 0.0, '365': 0.0, '366': 0.0, '367': 0.0, '368': 1.0, '369': 0.0, '370': 1.0, '371': 1.0, '372': 0.5, '373': 0.0, '374': 0.0, '375': 0.0, '376': 0.0, '377': 0.5, '378': 0.0, '379': 0.0, '380': 0.0, '381': 0.0, '382': 0.8, '383': 0.0, '384': 0.0, '385': 0.5, '386': 0.0, '387': 0.0, '388': 1.0, '389': 0.0, '390': 0.6, '391': 0.0, '392': 0.2, '393': 0.0, '394': 0.3333333333333333, '395': 0.0, '396': 0.6, '397': 0.0, '398': 0.2, '399': 1.0, '400': 0.5, '401': 0.0, '402': 0.0, '403': 0.2, '404': 0.0, '405': 0.25, '406': 0.75, '407': 0.5, '408': 0.0, '409': 0.0, '410': 0.0, '411': 0.0, '412': 0.2857142857142857, '413': 1.0, '414': 0.0, '415': 0.0, '416': 0.0}\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T17:13:13.673863Z",
     "start_time": "2024-05-22T17:13:13.648442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "def calculate_map(qrels_file: str, system_output_file: str) -> float:\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    average_precisions = []\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        total_relevant = sum(1 for doc_id, relevance in relevant_docs.items() if relevance > 0)\n",
    "        \n",
    "        if total_relevant == 0:\n",
    "            average_precisions.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        precision_sum = 0\n",
    "        num_relevant = 0\n",
    "        for rank, doc_id in enumerate(doc_ids, start=1):\n",
    "            if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0:\n",
    "                num_relevant += 1\n",
    "                precision = num_relevant / rank\n",
    "                precision_sum += precision\n",
    "        \n",
    "        average_precision = precision_sum / total_relevant\n",
    "        average_precisions.append(average_precision)\n",
    "    \n",
    "    map_value = sum(average_precisions) / len(average_precisions)\n",
    "    return map_value\n",
    "\n",
    "# Example usage\n",
    "map_value = calculate_map('secondqrels.csv', './second_query_with_doc_id_list2')\n",
    "print(f\"MAP: {map_value}\")"
   ],
   "id": "a446ceb591b6acc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.16250355902477073\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T08:21:35.531858Z",
     "start_time": "2024-06-03T08:21:35.454219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "def calculate_map(qrels_file: str, system_output_file: str) -> float:\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    average_precisions = []\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        total_relevant = sum(1 for doc_id, relevance in relevant_docs.items() if relevance > 0)\n",
    "        \n",
    "        if total_relevant == 0:\n",
    "            average_precisions.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        precision_sum = 0\n",
    "        num_relevant = 0\n",
    "        for rank, doc_id in enumerate(doc_ids, start=1):\n",
    "            if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0:\n",
    "                num_relevant += 1\n",
    "                precision = num_relevant / rank\n",
    "                precision_sum += precision\n",
    "        \n",
    "        average_precision = precision_sum / total_relevant\n",
    "        average_precisions.append(average_precision)\n",
    "    \n",
    "    map_value = sum(average_precisions) / len(average_precisions)\n",
    "    return map_value\n",
    "\n",
    "# Example usage\n",
    "map_value = calculate_map('secondqrels.csv', './lotte-query_result_without_cleaned_data.csv')\n",
    "print(f\"MAP: {map_value}\")"
   ],
   "id": "7a5568bcfe8d64f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.3892850498473144\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T14:07:12.842186Z",
     "start_time": "2024-06-02T14:07:12.741299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "def calculate_mrr(qrels_file: str, system_output_file: str) -> float:\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    mrr_values = []\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        \n",
    "        rank_of_first_relevant = float('inf')\n",
    "        for rank, doc_id in enumerate(doc_ids[:10], start=1):  # Consider only the top-10 results\n",
    "            if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0:\n",
    "                rank_of_first_relevant = rank\n",
    "                break\n",
    "        \n",
    "        if rank_of_first_relevant == float('inf'):\n",
    "            mrr_values.append(0.0)\n",
    "        else:\n",
    "            mrr_values.append(1 / rank_of_first_relevant)\n",
    "    \n",
    "    mrr_value = sum(mrr_values) / len(mrr_values)\n",
    "    return mrr_value\n",
    "\n",
    "# Example usage\n",
    "mrr_value = calculate_mrr('secondqrels.csv', './second_query_with_doc_id_list2')\n",
    "print(f\"MRR: {mrr_value}\")"
   ],
   "id": "fabac938d8dde0f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.29247268851585406\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T14:08:21.447708Z",
     "start_time": "2024-06-02T14:08:21.430420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "def calculate_precision_at_10(qrels_file: str, system_output_file: str) -> float:\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    precision_values = []\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        \n",
    "        num_relevant_in_top_10 = 0\n",
    "        for rank, doc_id in enumerate(doc_ids[:10], start=1):\n",
    "            if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0:\n",
    "                num_relevant_in_top_10 += 1\n",
    "        \n",
    "        precision_at_10 = num_relevant_in_top_10 / 10\n",
    "        precision_values.append(precision_at_10)\n",
    "    \n",
    "    precision_at_10 = sum(precision_values) / len(precision_values)\n",
    "    return precision_at_10\n",
    "\n",
    "# Example usage\n",
    "precision_at_10 = calculate_precision_at_10('secondqrels.csv', './second_query_with_doc_id_list2')\n",
    "print(f\"Precision at 10: {precision_at_10}\")"
   ],
   "id": "dcb1d2f183e15211",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision at 10: 0.08033573141486831\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T14:10:05.531852Z",
     "start_time": "2024-06-02T14:10:05.515619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "def calculate_recall_at_10(qrels_file: str, system_output_file: str) -> float:\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    recall_values = []\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        total_relevant_docs = sum(relevant_docs.values())\n",
    "        \n",
    "        num_relevant_in_top_10 = 0\n",
    "        for rank, doc_id in enumerate(doc_ids[:10], start=1):\n",
    "            if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0:\n",
    "                num_relevant_in_top_10 += 1\n",
    "        \n",
    "        recall_at_10 = num_relevant_in_top_10 / total_relevant_docs\n",
    "        recall_values.append(recall_at_10)\n",
    "    \n",
    "    recall_at_10 = sum(recall_values) / len(recall_values)\n",
    "    return recall_at_10\n",
    "\n",
    "# Example usage\n",
    "recall_at_10 = calculate_recall_at_10('secondqrels.csv', './second_query_with_doc_id_list2')\n",
    "print(f\"Recall at 10: {recall_at_10}\")"
   ],
   "id": "741b3654ce045ecd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at 10: 0.2896804301480561\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "177a283b812aff3e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
