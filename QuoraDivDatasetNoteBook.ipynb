{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ALL IMPORT",
   "id": "5ce001a0cad5a65c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import ir_datasets\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk import word_tokenize, PorterStemmer, pos_tag\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n"
   ],
   "id": "695921cd97437c3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "id": "2fd9e2cf1e6ea275",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "|INSTALL QUORA DATASET AND SAVE IT  USING PANDAS",
   "id": "31adfd07c1145668"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "dataset = ir_datasets.load(\"beir/quora/dev\")\n",
    "data = {'id': [],'title': []}\n",
    "for doc in dataset.docs_iter():\n",
    "    print(doc)\n",
    "    id = doc.doc_id\n",
    "    text = doc.text\n",
    "    data['id'].append(id)\n",
    "    data['title'].append(text)\n",
    "df = pd.DataFrame(data['title'], columns=['title'], index=data['id'])\n",
    "df.to_csv('./test2.csv')"
   ],
   "id": "f77d2f26454a376e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "|INSTALL QUORA Dataset Query AND SAVE IT  USING PANDAS",
   "id": "df84f7136e64a50d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = ir_datasets.load(\"beir/quora/dev\")\n",
    "data = {'id': [],'title': []}\n",
    "for doc in dataset.queries_iter():\n",
    "    print(doc)\n",
    "    id = doc.query_id\n",
    "    text = doc.text\n",
    "    data['id'].append(id)\n",
    "    data['title'].append(text)\n",
    "df = pd.DataFrame(data,columns=['id', 'title'])\n",
    "df.to_csv('./allQuery2.csv')"
   ],
   "id": "c7173d93ca90af12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "|INSTALL QUORA Dataset Qrels AND SAVE IT  USING PANDAS",
   "id": "b7eb40de68a91ccc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "dataset = ir_datasets.load(\"beir/quora/dev\")\n",
    "data = {'query_id': [], 'doc_id': [], 'relevance': [], 'iteration': []}\n",
    "for qrel in dataset.qrels_iter():\n",
    "    data['query_id'].append(qrel.query_id)\n",
    "    data['doc_id'].append(qrel.doc_id)\n",
    "    data['relevance'].append(qrel.relevance)\n",
    "    data['iteration'].append(qrel.iteration)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('./qrels.csv', index=False)"
   ],
   "id": "3bbb916c37ab34a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8d35c0ef17516b6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "260507bc9cb6e6fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "DATA PROCESSING CLASS",
   "id": "6971ac558f528452"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stop_words = stopwords.words('english')\n",
    "class TextProcessor:\n",
    "    @staticmethod\n",
    "    def lower(text):\n",
    "         if isinstance(text, str):  # التحقق من أن القيمة من نوع str\n",
    "            return text.lower()\n",
    "         else:\n",
    "            return \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def     tokenize_text(text):\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_stopwords(text, stop_words=stop_words):\n",
    "       return [word for word in text if not word in stop_words]\n",
    "    \n",
    "\n",
    " \n",
    "    @staticmethod\n",
    "    def remove_punc(text):\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in text]\n",
    "        # print(\"remove punctuation:\", stripped)\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        return words\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_wordnet_pos(tag_parameter):\n",
    "     tag = tag_parameter[0].upper()\n",
    "     tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "     return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def lemetize_Text(text):\n",
    "     pos_tags = nltk.pos_tag(text)\n",
    "     lemmatizer = WordNetLemmatizer()\n",
    "     lemmatized_words = [lemmatizer.lemmatize(text, pos=TextProcessor.get_wordnet_pos(pos_tag)) for text, pos_tag in pos_tags]\n",
    "     return lemmatized_words\n",
    "    \n",
    "    @staticmethod\n",
    "    def stemming(text):\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(word) for word in text]\n",
    "    @staticmethod\n",
    "    def process(text):\n",
    "        text_processed = TextProcessor.lower(text)\n",
    "        text_processed = TextProcessor.tokenize_text(text_processed)\n",
    "        text_processed = TextProcessor.remove_stopwords(text_processed)\n",
    "        text_processed = TextProcessor.remove_punc(text_processed)\n",
    "        text_processed = TextProcessor.lemetize_Text(text_processed)\n",
    "        text_processed = TextProcessor.stemming(text_processed)\n",
    "        return text_processed"
   ],
   "id": "57182546ba176b6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.read_csv('./test2.csv') ",
   "id": "cbb9c1f6dbc82127",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def clean_title(title):\n",
    "    processed_title = TextProcessor.process(title) \n",
    "    cleaned_title = ' '.join(processed_title)  \n",
    "    return cleaned_title\n",
    "df['title'] = df['title'].apply(clean_title) \n",
    "df.to_csv('./cleanedDataset3.csv', index=False) "
   ],
   "id": "4e20d18d27884199",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "345fcb7a6efce5f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "|FUNCTION FOR SAVE AND READ AND LOAD",
   "id": "9b32cbcfef6d5bec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def read_documents(directory):\n",
    "    documents = []\n",
    "    document_names = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename), 'r' ,encoding='utf-8') as file:\n",
    "            document = file.read()\n",
    "            documents.append(document)\n",
    "            document_names.append(filename)\n",
    "\n",
    "    return documents, document_names\n",
    "\n",
    "def read_documents_from_one_file(filePath):\n",
    "    df2 = pd.read_csv(filePath)\n",
    "    documents2 = df2['title'].fillna('', inplace=False).tolist()\n",
    "    document_names2 = df2['Unnamed: 0'].tolist()\n",
    "    return documents2, document_names2\n",
    "\n",
    "  \n",
    "\n",
    "def save_vectorizer(vectorizer, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "\n",
    "def save_tfidf_matrix(tfidf_matrix, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(tfidf_matrix, f)\n",
    "\n",
    "def save_document_names(document_names, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(document_names, f)\n",
    "\n",
    "def save_inverted_index(inverted_index, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(inverted_index, f)\n",
    "\n",
    "def load_vectorizer(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "# \n",
    "def load_tfidf_matrix(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_document_names(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ],
   "id": "f00cc555ffd0d16b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "VECTORIZER TFIDF AND SAVE IT TO PICKLE FILES\n",
   "id": "794c7499fbeaab3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "directory = \"./cleanedDataset3.csv\"\n",
    "documents2, document_names2 = read_documents_from_one_file(directory)\n",
    "print(documents2)\n",
    "vectorizer = TfidfVectorizer()\n",
    "print(\"Vectorizer Created.\")\n",
    "tfidf_matrix = vectorizer.fit_transform(documents2)\n",
    "print(\"TF-IDF Created.\")\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "save_vectorizer(vectorizer, \"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\vectorizer.pkl\")\n",
    "save_tfidf_matrix(tfidf_matrix, \"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\tfidf_matrix.pkl\")\n",
    "save_document_names(document_names2, \"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\document_names.pkl\")"
   ],
   "id": "cc126f660f05393c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "LOAD VICTORAIZE FILES AND MATCHING WITHOUT DOCUMENT CONTENT\n",
    "\n"
   ],
   "id": "a55478f3153bec79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loaded_vectorizer = load_vectorizer(\"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\vectorizer.pkl\")\n",
    "loaded_tfidf_matrix = load_tfidf_matrix(\"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\tfidf_matrix.pkl\")\n",
    "loaded_document_names = load_document_names(\"C:\\\\Users\\\\Ghost\\\\Desktop\\\\pickleFiles\\\\document_names.pkl\")                           "
   ],
   "id": "b34fbc6dea818ba8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def query_matching(query: str, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names):\n",
    "    query2=TextProcessor.process(query)\n",
    "    query3=' '.join(query2)\n",
    "    query_tfidf = loaded_vectorizer.transform([query3])\n",
    "    query_vector = query_tfidf\n",
    "    similarity = cosine_similarity(query_vector, loaded_tfidf_matrix).flatten()\n",
    "    # similarity1=np.argsort(similarity)[::-1]\n",
    "    matching_results = []\n",
    "    for i, doc_name in enumerate(loaded_document_names):\n",
    "        if similarity[i] > 0:\n",
    "            matching_results.append((doc_name, similarity[i]))\n",
    "    sorted_results = sorted(matching_results, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_results\n",
    "\n",
    "# query = \"How does Quora look to a moderator\"\n",
    "# query2=TextProcessor.process(query)\n",
    "# query3=' '.join(query2)\n",
    "# print(query3)\n",
    "# matching_results = query_matching(query3, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names)\n",
    "# for doc_name, similarity in matching_results:\n",
    "#     print(f\"Document ID: {doc_name}\")\n",
    "#     print(f\"Similarity: {similarity}\\n\")"
   ],
   "id": "f4e1c5c63ae188d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def retrieve_files(query: str, file_path: str, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names):\n",
    "    retrieved_files = []\n",
    "    query_tfidf = loaded_vectorizer.transform([query])\n",
    "    query_vector = query_tfidf.toarray()[0]\n",
    "    similarity = cosine_similarity(query_vector.reshape(1, -1), loaded_tfidf_matrix).flatten()\n",
    "    sorted_indices = np.argsort(similarity)[::-1]  # عكس ترتيب الفهارس للترتيب التنازلي\n",
    "    sorted_documents = [(loaded_document_names[i], similarity[i]) for i in sorted_indices if similarity[i] > 0]\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(',')\n",
    "            document = parts[0]\n",
    "            content = ','.join(parts[1:])\n",
    "            retrieved_files.append((document, content))\n",
    "\n",
    "    merged_results = []\n",
    "    for document, content in retrieved_files:\n",
    "        for doc_name, doc_similarity in sorted_documents:\n",
    "            if document == doc_name:\n",
    "                merged_results.append((document, content, doc_similarity))\n",
    "                break\n",
    "    return merged_results\n",
    "file_path = \"./test2.csv\"  # استبدله بمسار الملف الفعلي\n",
    "results = retrieve_files(\"internet\", file_path, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names)\n",
    "for document, content, similarity in results:\n",
    "    print(f\"Document: {document}\")\n",
    "    print(f\"Content: {content}\")\n",
    "    print(f\"Similarity: {similarity}\\n\")"
   ],
   "id": "df6d766ea1767065",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Fetch Top 10 Sorted By Similarity",
   "id": "45b16edd9987ff3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def query_matching_top_10(query: str, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names):\n",
    "    query2 = TextProcessor.process(query)\n",
    "    query3 = ' '.join(query2)\n",
    "    query_tfidf = loaded_vectorizer.transform([query3])\n",
    "    query_vector = query_tfidf\n",
    "    similarity = cosine_similarity(query_vector, loaded_tfidf_matrix).flatten()\n",
    "    matching_results = []\n",
    "    for i, doc_name in enumerate(loaded_document_names):\n",
    "        if similarity[i] > 0:\n",
    "            matching_results.append((doc_name, similarity[i]))\n",
    "    sorted_results = sorted(matching_results, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return only the top 10 results\n",
    "    return sorted_results[:10]"
   ],
   "id": "5e532cc7b3d1044",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "matching=query_matching_top_10(\"step step guid invest share market india?\", loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names)\n",
    "print(matching)"
   ],
   "id": "89a810c93e09faac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_queries(file_path, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names):\n",
    "    queries_df = pd.read_csv(file_path)\n",
    "    doc_names_list = []\n",
    "    query_id_results = {}\n",
    "    for index, row in queries_df.iterrows():\n",
    "        query_id = row['id']\n",
    "        query = row['title'].strip()\n",
    "        matching_results = query_matching_top_10(query, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names)\n",
    "        doc_names = [doc_name for doc_name, similarity in matching_results]\n",
    "        doc_names_list.extend(doc_names)\n",
    "        query_id_results[query_id] = {'Query': query, 'Document Names': doc_names}\n",
    "    query_results_df = pd.DataFrame.from_dict(query_id_results, orient='index')\n",
    "    query_results_df.to_csv('query_results_with_document_id3.csv', index_label='Query ID')\n",
    "file_path = './allQuery2.csv'\n",
    "process_queries(file_path, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names)"
   ],
   "id": "af33d48a6d69bb39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Process All Query with Dataset And Save The Result",
   "id": "3a7c4c6b2e5a0730"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_queries_without_query_text(file_path, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names):\n",
    "    queries_df = pd.read_csv(file_path)\n",
    "    doc_names_list = []\n",
    "    query_id_results = {}\n",
    "    for index, row in queries_df.iterrows():\n",
    "        query_id = row['id']\n",
    "        query = row['title'].strip()\n",
    "        matching_results = query_matching_top_10(query, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names)\n",
    "        doc_names = [doc_name for doc_name, similarity in matching_results]\n",
    "        doc_names_list.extend(doc_names)\n",
    "        query_id_results[query_id] = {'doc_id': doc_names}\n",
    "    query_results_df = pd.DataFrame.from_dict(query_id_results, orient='index')\n",
    "    query_results_df.to_csv('query_with_doc_id_list', index_label='Query ID')\n",
    "file_path = './allQuery2.csv'\n",
    "process_queries_without_query_text(file_path, loaded_vectorizer, loaded_tfidf_matrix, loaded_document_names)"
   ],
   "id": "764a21d514e8dde1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate Precision  WITH TFIDF",
   "id": "58b4b1ed2fb79ebb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from typing import Dict\n",
    "\n",
    "def calculate_precision_at_k(qrels_file: str, system_output_file: str, k=10):\n",
    "\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    precision_at_k = {}\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        num_relevant = sum(1 for doc_id in doc_ids[:k] if str(doc_id) in relevant_docs)\n",
    "        precision_at_k[query_id] = num_relevant / k\n",
    "    \n",
    "    return precision_at_k\n",
    "\n",
    "# Example usage\n",
    "precision_at_10 = calculate_precision_at_k('qrels.csv', './query_with_doc_id_list', k=10)\n",
    "print(precision_at_10)"
   ],
   "id": "c4ad2e7b6c255e5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "Calculate Precision With Embedding Model Rresult",
   "id": "e0fbc0a0643254ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from typing import Dict\n",
    "\n",
    "def calculate_precision_at_k(qrels_file: str, system_output_file: str, k=10):\n",
    "\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    precision_at_k = {}\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        num_relevant = sum(1 for doc_id in doc_ids[:k] if str(doc_id) in relevant_docs)\n",
    "        precision_at_k[query_id] = num_relevant / k\n",
    "    \n",
    "    return precision_at_k\n",
    "\n",
    "# Example usage\n",
    "precision_at_10 = calculate_precision_at_k('qrels.csv', './query_result_without_cleaned_data.csv', k=10)\n",
    "print(precision_at_10)"
   ],
   "id": "dce2ff2a1031250b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from typing import Dict\n",
    "\n",
    "def calculate_recall_at_k(qrels_file: str, system_output_file: str, k=10):\n",
    " \n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    recall_at_k = {}\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        total_relevant = sum(1 for doc_id, relevance in relevant_docs.items() if relevance > 0)\n",
    "        num_relevant = sum(1 for doc_id in doc_ids[:k] if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0)\n",
    "        recall_at_k[query_id] = num_relevant / total_relevant\n",
    "    \n",
    "    return recall_at_k\n",
    "\n",
    "# Example usage\n",
    "recall_at_10 = calculate_recall_at_k('qrels.csv', './query_with_doc_id_list', k=10)\n",
    "print(recall_at_10)"
   ],
   "id": "fe32abc08b6f6f74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T21:02:57.145030Z",
     "start_time": "2024-06-03T21:02:57.070230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "def calculate_map(qrels_file: str, system_output_file: str) -> float:\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    average_precisions = []\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        total_relevant = sum(1 for doc_id, relevance in relevant_docs.items() if relevance > 0)\n",
    "        \n",
    "        if total_relevant == 0:\n",
    "            average_precisions.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        precision_sum = 0\n",
    "        num_relevant = 0\n",
    "        for rank, doc_id in enumerate(doc_ids, start=1):\n",
    "            if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0:\n",
    "                num_relevant += 1\n",
    "                precision = num_relevant / rank\n",
    "                precision_sum += precision\n",
    "        \n",
    "        average_precision = precision_sum / total_relevant\n",
    "        average_precisions.append(average_precision)\n",
    "    \n",
    "    map_value = sum(average_precisions) / len(average_precisions)\n",
    "    return map_value\n",
    "\n",
    "# Example usage\n",
    "map_value = calculate_map('qrels.csv', './query_with_doc_id_list')\n",
    "print(f\"MAP: {map_value}\")"
   ],
   "id": "b642f3b19be90ab3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.6903602725211702\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "def calculate_map(qrels_file: str, system_output_file: str) -> float:\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    average_precisions = []\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        total_relevant = sum(1 for doc_id, relevance in relevant_docs.items() if relevance > 0)\n",
    "        \n",
    "        if total_relevant == 0:\n",
    "            average_precisions.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        precision_sum = 0\n",
    "        num_relevant = 0\n",
    "        for rank, doc_id in enumerate(doc_ids, start=1):\n",
    "            if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0:\n",
    "                num_relevant += 1\n",
    "                precision = num_relevant / rank\n",
    "                precision_sum += precision\n",
    "        \n",
    "        average_precision = precision_sum / total_relevant\n",
    "        average_precisions.append(average_precision)\n",
    "    \n",
    "    map_value = sum(average_precisions) / len(average_precisions)\n",
    "    return map_value\n",
    "\n",
    "# Example usage\n",
    "map_value = calculate_map('qrels.csv', './qoura_embiding_query_result2.csv')\n",
    "print(f\"MAP: {map_value}\")"
   ],
   "id": "29a98af0e5abaecf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate Map With Embedding MODEL",
   "id": "34dc8fa80632dfe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T21:02:51.509821Z",
     "start_time": "2024-06-03T21:02:51.400542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "def calculate_map(qrels_file: str, system_output_file: str) -> float:\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    average_precisions = []\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        total_relevant = sum(1 for doc_id, relevance in relevant_docs.items() if relevance > 0)\n",
    "        \n",
    "        if total_relevant == 0:\n",
    "            average_precisions.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        precision_sum = 0\n",
    "        num_relevant = 0\n",
    "        for rank, doc_id in enumerate(doc_ids, start=1):\n",
    "            if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0:\n",
    "                num_relevant += 1\n",
    "                precision = num_relevant / rank\n",
    "                precision_sum += precision\n",
    "        \n",
    "        average_precision = precision_sum / total_relevant\n",
    "        average_precisions.append(average_precision)\n",
    "    \n",
    "    map_value = sum(average_precisions) / len(average_precisions)\n",
    "    return map_value\n",
    "\n",
    "# Example usage\n",
    "map_value = calculate_map('qrels.csv', './query_result_with_embedding.csv')\n",
    "print(f\"MAP: {map_value}\")"
   ],
   "id": "91c4009986b6f0f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.8473709961427701\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "CALCULATE MRR WITH EMBEDDING MODEL",
   "id": "8f12789f2a466323"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "def calculate_mrr(qrels_file: str, system_output_file: str) -> float:\n",
    "    qrels = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_id, relevance, _ = row\n",
    "            if query_id not in qrels:\n",
    "                qrels[query_id] = {}\n",
    "            qrels[query_id][doc_id] = int(relevance)\n",
    "    \n",
    "    system_output = {}\n",
    "    with open(system_output_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            query_id, doc_ids = row\n",
    "            doc_id_list = [int(doc_id.strip()) for doc_id in doc_ids.strip('[]').split(', ') if doc_id.strip()]\n",
    "            system_output[query_id] = doc_id_list\n",
    "    \n",
    "    mrr_values = []\n",
    "    for query_id, doc_ids in system_output.items():\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        \n",
    "        rank_of_first_relevant = float('inf')\n",
    "        for rank, doc_id in enumerate(doc_ids, start=1):\n",
    "            if str(doc_id) in relevant_docs and relevant_docs[str(doc_id)] > 0:\n",
    "                rank_of_first_relevant = rank\n",
    "                break\n",
    "        \n",
    "        if rank_of_first_relevant == float('inf'):\n",
    "            mrr_values.append(0.0)\n",
    "        else:\n",
    "            mrr_values.append(1 / rank_of_first_relevant)\n",
    "    \n",
    "    mrr_value = sum(mrr_values) / len(mrr_values)\n",
    "    return mrr_value\n",
    "\n",
    "# Example usage\n",
    "mrr_value = calculate_mrr('qrels.csv', './query_result_with_embedding.csv')\n",
    "print(f\"MRR: {mrr_value}\")"
   ],
   "id": "535a476aa88c6a0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b91a544be0846d43",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
